# ğŸ“ Project Structure

## Complete File Overview

```
new testing scripts/
â”‚
â”œâ”€â”€ ğŸ“„ Core Application Files
â”‚   â”œâ”€â”€ app.py                     # Main Streamlit application (UI + routing)
â”‚   â”œâ”€â”€ config.py                  # Configuration settings and constants
â”‚   â”œâ”€â”€ database.py                # SQLite database operations (async)
â”‚   â”œâ”€â”€ sitemap_crawler.py         # Sitemap discovery and parsing (async)
â”‚   â”œâ”€â”€ link_extractor.py          # Outgoing link extraction (async)
â”‚   â””â”€â”€ utils.py                   # Utility functions and helpers
â”‚
â”œâ”€â”€ ğŸ“‹ Setup & Configuration
â”‚   â”œâ”€â”€ requirements.txt           # Python package dependencies
â”‚   â”œâ”€â”€ .env.example              # Environment variables template
â”‚   â”œâ”€â”€ .gitignore                # Git ignore patterns
â”‚   â”œâ”€â”€ setup.bat                 # Windows setup script
â”‚   â”œâ”€â”€ setup.sh                  # Linux/Mac setup script
â”‚   â”œâ”€â”€ run.bat                   # Windows launcher
â”‚   â””â”€â”€ run.sh                    # Linux/Mac launcher
â”‚
â”œâ”€â”€ ğŸ“– Documentation
â”‚   â”œâ”€â”€ README.md                 # Complete documentation
â”‚   â”œâ”€â”€ QUICKSTART.md             # Quick start guide
â”‚   â””â”€â”€ PROJECT_STRUCTURE.md      # This file
â”‚
â”œâ”€â”€ ğŸ—„ï¸ Data Storage (created on first run)
â”‚   â”œâ”€â”€ crawler_data.db           # SQLite database
â”‚   â”œâ”€â”€ logs/                     # Log files directory
â”‚   â”‚   â”œâ”€â”€ crawler.log          # General application logs
â”‚   â”‚   â””â”€â”€ errors.log           # Error logs
â”‚   â””â”€â”€ exports/                  # Exported data files
â”‚       â”œâ”€â”€ *.csv                # CSV exports
â”‚       â”œâ”€â”€ *.xlsx               # Excel exports
â”‚       â””â”€â”€ *.json               # JSON exports
â”‚
â””â”€â”€ ğŸ”§ Runtime Files (created during execution)
    â””â”€â”€ .streamlit/               # Streamlit configuration (auto-created)
```

## File Descriptions

### Core Application Files

#### app.py (Main Application)
- **Purpose**: Streamlit web interface
- **Pages**: 
  - Dashboard (statistics and overview)
  - Manage Sites (add/edit/delete sites)
  - Sitemap Crawler (discover and parse sitemaps)
  - Link Extractor (extract outgoing links)
  - Export Data (export to CSV/Excel/JSON)
  - Settings (configuration management)
- **Features**:
  - Real-time progress tracking
  - Professional UI with light theme
  - Async operation support
  - Error handling and logging

#### config.py (Configuration)
- **Purpose**: Central configuration management
- **Settings**:
  - Database path and configuration
  - Crawler settings (threads, timeout, retries)
  - Sitemap patterns (WordPress, Yoast, RankMath)
  - Post URL patterns (blogs, articles, news)
  - Excluded domains (social media, trackers)
  - UI configuration (colors, layout)
  - Export settings

#### database.py (Database Layer)
- **Purpose**: SQLite database operations
- **Tables**:
  - `sites` - Website information
  - `sitemap_urls` - Discovered sitemap URLs
  - `posts` - Individual post URLs
  - `outgoing_links` - Extracted links with metadata
  - `crawl_history` - Crawl operation logs
  - `error_logs` - Error tracking
  - `settings` - Application settings
  - `excluded_domains` - Domain exclusion list
  - `custom_sitemaps` - Custom sitemap patterns
  - `post_patterns` - Custom post URL patterns
- **Features**:
  - Async operations with aiosqlite
  - Proper indexing for performance
  - Comprehensive error handling
  - Transaction management

#### sitemap_crawler.py (Sitemap Discovery)
- **Purpose**: Discover and parse website sitemaps
- **Capabilities**:
  - Auto-discovery using common patterns
  - robots.txt checking
  - Sitemap index handling (nested sitemaps)
  - URL filtering (posts vs pages)
  - Async/concurrent crawling
  - Retry logic with backoff
- **Functions**:
  - `discover_sitemap()` - Find sitemap URL
  - `parse_sitemap()` - Extract URLs from sitemap
  - `get_all_sitemap_urls()` - Recursively get all URLs
  - `check_robots_txt()` - Check robots.txt for sitemaps
  - `discover_multiple_sites()` - Batch discovery

#### link_extractor.py (Link Extraction)
- **Purpose**: Extract outgoing links from web pages
- **Features**:
  - Content area detection (article, main, etc.)
  - Link position calculation (paragraph, word count)
  - Link classification (article, sidebar, h2, etc.)
  - External link filtering
  - Domain exclusion
  - Attribute tracking (rel, target)
  - Playwright fallback for JavaScript sites
- **Functions**:
  - `extract_outgoing_links()` - Extract from single page
  - `extract_from_multiple_pages()` - Batch extraction
  - `batch_extract_with_fallback()` - Auto-fallback to Playwright
- **Modes**:
  - Fast mode: aiohttp + BeautifulSoup
  - Deep mode: Playwright (JavaScript rendering)

#### utils.py (Utilities)
- **Purpose**: Helper functions and utilities
- **Functions**:
  - `setup_logging()` - Configure logging system
  - `run_async()` - Run async code in sync context
  - `format_timestamp()` - Format dates for display
  - `format_number()` - Format numbers with commas
  - `validate_url()` - URL validation
  - `export_to_csv()` - CSV export
  - `export_to_excel()` - Excel export
  - `export_to_json()` - JSON export
  - `export_multiple_sheets()` - Multi-sheet Excel
  - `ProgressTracker` - Progress tracking class

### Setup & Configuration Files

#### requirements.txt
All Python dependencies:
- streamlit (UI framework)
- aiohttp (async HTTP)
- beautifulsoup4 (HTML parsing)
- playwright (browser automation)
- pandas (data manipulation)
- openpyxl (Excel support)
- validators (URL validation)
- aiosqlite (async SQLite)

#### .env.example
Template for environment variables (copy to `.env` and customize)

#### setup.bat / setup.sh
One-time setup scripts to install all dependencies

#### run.bat / run.sh
Launch scripts to start the application

### Documentation Files

#### README.md
Complete documentation including:
- Features overview
- Installation instructions
- Usage guide
- Configuration options
- Troubleshooting
- Best practices

#### QUICKSTART.md
Quick start guide for new users:
- Installation steps
- First-time setup
- Common tasks
- Performance tips
- Troubleshooting basics

#### PROJECT_STRUCTURE.md
This file - complete project structure documentation

## Database Schema

### sites
```sql
id, url, sitemap_url, status, total_posts, total_outgoing_links,
created_at, last_crawled_at, last_updated_at, notes
```

### posts
```sql
id, site_id, url, title, status, outgoing_links_count,
crawled_at, created_at
```

### outgoing_links
```sql
id, post_id, site_id, target_url, anchor_text,
position_paragraph, position_word, link_location,
rel_attributes, target_attribute, is_article_link, created_at
```

### crawl_history
```sql
id, site_id, crawl_type, status, new_posts_found, new_links_found,
errors_count, started_at, completed_at, error_message
```

### error_logs
```sql
id, site_id, post_url, error_type, error_message,
retry_count, created_at, resolved
```

## Data Flow

```
1. User adds site URL
   â†“
2. Sitemap Crawler discovers sitemap
   â†“
3. Parser extracts all post URLs
   â†“
4. URLs stored in database
   â†“
5. Link Extractor fetches each post
   â†“
6. Outgoing links extracted & stored
   â†“
7. Data exported to CSV/Excel/JSON
```

## Async Architecture

```
Main Thread (Streamlit UI)
    â†“
run_async() wrapper
    â†“
AsyncIO Event Loop
    â†“
Concurrent Tasks (20-100 simultaneous)
    â†“
aiohttp/Playwright fetching
    â†“
Database operations (aiosqlite)
    â†“
Progress callbacks
    â†“
UI updates
```

## Performance Characteristics

### Sitemap Discovery
- **Speed**: 1-5 seconds per site
- **Concurrent**: Up to 10 sites simultaneously
- **Memory**: ~50MB per site

### Link Extraction
- **Speed**: 0.5-2 seconds per post (async mode)
- **Speed**: 2-5 seconds per post (Playwright mode)
- **Concurrent**: 20-100 threads
- **Memory**: ~100-500MB depending on threads

### Database Operations
- **Read**: < 10ms for most queries
- **Write**: Batched for efficiency
- **Size**: ~1KB per post, ~500 bytes per link

## Scalability

### Small Scale (1-10 sites, <1000 posts)
- Threads: 10-20
- Memory: 1-2GB
- Time: Minutes

### Medium Scale (10-100 sites, 1000-10000 posts)
- Threads: 20-50
- Memory: 2-4GB
- Time: Hours

### Large Scale (100+ sites, 10000+ posts)
- Threads: 50-100
- Memory: 4-8GB
- Time: Hours to days
- Recommendation: Process in batches

## Security & Privacy

- All data stored locally
- No external API calls except to target websites
- Database encrypted if needed (user responsibility)
- No telemetry or tracking
- Open source and auditable

## Maintenance

### Regular Tasks
- Clear old logs (monthly)
- Backup database (weekly)
- Update excluded domains (as needed)
- Review error logs (after each crawl)

### Optimization
- Vacuum database (monthly): `VACUUM;`
- Re-index (if slow): `REINDEX;`
- Clear exports (as needed)

## Future Enhancements

Potential additions:
- Multi-database support (PostgreSQL, MySQL)
- API endpoints (REST/GraphQL)
- Scheduled crawling
- Email notifications
- Data visualization (charts, graphs)
- Advanced filtering and search
- Duplicate content detection
- Link relationship mapping

---

**Version**: 1.0.0  
**Last Updated**: January 2026  
**Status**: Production Ready
